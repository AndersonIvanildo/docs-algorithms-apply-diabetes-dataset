{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introdu\u00e7\u00e3o Essa p\u00e1gina foi um projeto criado por mim para explorar a aplica\u00e7\u00e3o e o aprimoramento de algoritmos em um projeto focado no diagn\u00f3stico diabetes. Ele servir\u00e1 n\u00e3o apenas para relatar a implementa\u00e7\u00e3o em si, mas pela oportunidade de revisitar e aprofundar meu conhecimento sobre as t\u00e9cnicas e algoritmos empregados. Motiva\u00e7\u00e3o A ideia deste projeto nasceu de uma necessidade pr\u00f3pria de ter um caderno de bolso pr\u00e1tico e acess\u00edvel. Al\u00e9m disso, servir\u00e1 como um compilado das t\u00e9cnicas que utilizei em diferentes reposit\u00f3rios evitando ter que acessar cada projeto individualmente.","title":"In\u00edcio"},{"location":"#introducao","text":"Essa p\u00e1gina foi um projeto criado por mim para explorar a aplica\u00e7\u00e3o e o aprimoramento de algoritmos em um projeto focado no diagn\u00f3stico diabetes. Ele servir\u00e1 n\u00e3o apenas para relatar a implementa\u00e7\u00e3o em si, mas pela oportunidade de revisitar e aprofundar meu conhecimento sobre as t\u00e9cnicas e algoritmos empregados.","title":"Introdu\u00e7\u00e3o"},{"location":"#motivacao","text":"A ideia deste projeto nasceu de uma necessidade pr\u00f3pria de ter um caderno de bolso pr\u00e1tico e acess\u00edvel. Al\u00e9m disso, servir\u00e1 como um compilado das t\u00e9cnicas que utilizei em diferentes reposit\u00f3rios evitando ter que acessar cada projeto individualmente.","title":"Motiva\u00e7\u00e3o"},{"location":"the_dataset/","text":"Descri\u00e7\u00e3o do Dataset O conjunto de dados utilizado neste projeto foi obtido na plataforma Kaggle, disponibilizado pelo autor Marshal Patel. Este dataset \u00e9 composto por informa\u00e7\u00f5es biom\u00e9dicas e caracter\u00edsticas de pacientes, sendo fundamental para a cria\u00e7\u00e3o de modelos de predi\u00e7\u00e3o de diabetes. O dataset pode ser acessado clicado nesse link Link do Dataset : Descri\u00e7\u00e3o dos Par\u00e2metros Abaixo, segue uma descri\u00e7\u00e3o detalhada de cada um dos par\u00e2metros (colunas) presentes no conjunto de dados: Gender: O g\u00eanero do paciente (Feminino ou Masculino). AGE: A idade do paciente em anos. Urea: O n\u00edvel de ureia no sangue, um indicador da fun\u00e7\u00e3o renal. Cr (Creatinina): O n\u00edvel de creatinina no sangue, outro importante indicador da fun\u00e7\u00e3o dos rins. HbA1c (Hemoglobina Glicada): Um exame que mede a m\u00e9dia dos n\u00edveis de a\u00e7\u00facar no sangue nos \u00faltimos 2 a 3 meses. \u00c9 um par\u00e2metro chave para o diagn\u00f3stico e monitoramento do diabetes. Chol (Colesterol): O n\u00edvel de colesterol total no sangue. TG (Triglicer\u00eddeos): O n\u00edvel de triglicer\u00eddeos no sangue, um tipo de gordura presente na corrente sangu\u00ednea. HDL (Lipoprote\u00edna de Alta Densidade): Conhecido como \"colesterol bom\", ajuda a remover o excesso de colesterol do corpo. LDL (Lipoprote\u00edna de Baixa Densidade): Conhecido como \"colesterol ruim\", seu ac\u00famulo pode levar \u00e0 forma\u00e7\u00e3o de placas nas art\u00e9rias. VLDL (Lipoprote\u00edna de Muito Baixa Densidade): Um precursor do LDL, tamb\u00e9m associado ao ac\u00famulo de placas nas art\u00e9rias. BMI (\u00cdndice de Massa Corporal): Uma medida da gordura corporal baseada na altura e no peso do indiv\u00edduo. CLASS: A classe de diagn\u00f3stico do paciente, podendo ser N (N\u00e3o diab\u00e9tico), P (Pr\u00e9-diab\u00e9tico) ou Y (Diab\u00e9tico). Este dataset foi utilizado em todas as t\u00e9cnicas do meu projeto como base para explorar as possibilidades. An\u00e1lise de Dados e Prepara\u00e7\u00e3o para Modelagem de Predi\u00e7\u00e3o de Diabetes Aqui eu fa\u00e7o uma descri\u00e7\u00e3o que descreve o processo completo de an\u00e1lise e pr\u00e9-processamento de um dataset de predi\u00e7\u00e3o de diabetes. O objetivo \u00e9 preparar os dados de forma robusta, criando uma base s\u00f3lida e confi\u00e1vel para o treinamento de diferentes algoritmos de Machine Learning capazes de classificar pacientes como n\u00e3o-diab\u00e9ticos (N), pr\u00e9-diab\u00e9ticos (P) ou diab\u00e9ticos (Y). 1. Aquisi\u00e7\u00e3o e Explora\u00e7\u00e3o dos Dados O primeiro passo em qualquer projeto de ci\u00eancia de dados \u00e9 obter e entender o conjunto de dados com o qual estamos trabalhando. 1.1. Carregamento do Dataset Utilizei a biblioteca opendatasets para baixar o conjunto de dados diretamente da plataforma Kaggle e o pandas para carreg\u00e1-lo em um DataFrame, que \u00e9 a estrutura de dados fundamental para an\u00e1lise em Python. import opendatasets as od import pandas as pd od.download(\"https://www.kaggle.com/datasets/marshalpatel3558/diabetes-prediction-dataset-legit-dataset\") 1.2. An\u00e1lise Explorat\u00f3ria Inicial Com os dados carregados, realizei uma an\u00e1lise explorat\u00f3ria inicial para entender sua estrutura, tipos de dados e a presen\u00e7a de valores ausentes. .head() : Visualizei as 5 primeiras linhas para ter uma primeira impress\u00e3o dos dados e das colunas. diabetes_dataset.head(5) ID No_Pation Gender AGE Urea Cr HbA1c Chol TG HDL LDL VLDL BMI CLASS 0 502 17975 F 50 4.7 46 4.9 4.2 0.9 2.4 1.4 0.5 24.0 N 1 735 34221 M 26 4.5 62 4.9 3.7 1.4 1.1 2.1 0.6 23.0 N 2 420 47975 F 50 4.7 46 4.9 4.2 0.9 2.4 1.4 0.5 24.0 N 3 680 87656 F 50 4.7 46 4.9 4.2 0.9 2.4 1.4 0.5 24.0 N 4 504 34223 M 33 7.1 46 4.9 4.9 1.0 0.8 2.0 0.4 21.0 N .info() : Verifiquei os tipos de dados de cada coluna e a contagem de valores n\u00e3o-nulos. Felizmente, o dataset n\u00e3o apresentou valores ausentes, o que simplifica a etapa de limpeza. diabetes_dataset.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 1000 entries, 0 to 999 Data columns (total 14 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 1000 non-null int64 1 No_Pation 1000 non-null int64 2 Gender 1000 non-null object 3 AGE 1000 non-null int64 4 Urea 1000 non-null float64 5 Cr 1000 non-null int64 6 HbA1c 1000 non-null float64 7 Chol 1000 non-null float64 8 TG 1000 non-null float64 9 HDL 1000 non-null float64 10 LDL 1000 non-null float64 11 VLDL 1000 non-null float64 12 BMI 1000 non-null float64 13 CLASS 1000 non-null object 2. Pr\u00e9-processamento e Limpeza dos Dados A qualidade do modelo de Machine Learning depende diretamente da qualidade dos dados de entrada. Nesta etapa, corrigi inconsist\u00eancias e transformei os dados para um formato adequado para o treinamento. 2.1. Limpeza de Dados Categ\u00f3ricos identifiquei que as colunas categ\u00f3ricas Gender e CLASS continham ru\u00eddos, como valores diferentes para a mesma categoria (ex: 'N' e 'N ') e inconsist\u00eancia de mai\u00fasculas/min\u00fasculas (ex: 'F' e 'f'). # Verificando os valores \u00fanicos antes da limpeza list_class = diabetes_dataset['CLASS'].unique() list_gender = diabetes_dataset['Gender'].unique() print(f\"For label CLASS: {list_class}\\nFor label Gender: {list_gender}\") # Sa\u00edda: # For label CLASS: ['N' 'N ' 'P' 'Y' 'Y '] # For label Gender: ['F' 'M' 'f'] Para corrigir, apliquei uma fun\u00e7\u00e3o para converter todos os valores para mai\u00fasculas e remover espa\u00e7os em branco no in\u00edcio e no fim das strings. # Aplicando a limpeza diabetes_dataset['CLASS'] = diabetes_dataset['CLASS'].apply(lambda x: str.upper(x).strip()) diabetes_dataset['Gender'] = diabetes_dataset['Gender'].apply(lambda x: str.upper(x)) # Verificando o resultado ap\u00f3s a limpeza list_class = diabetes_dataset['CLASS'].unique() list_gender = diabetes_dataset['Gender'].unique() print(f\"For label CLASS: {list_class}\\nFor label Gender: {list_gender}\") # Sa\u00edda: # For label CLASS: ['N' 'P' 'Y'] # For label Gender: ['F' 'M'] 2.2. Remo\u00e7\u00e3o de Colunas Irrelevantes As colunas ID e No_Pation s\u00e3o identificadores \u00fanicos para cada paciente. Elas n\u00e3o possuem valor preditivo e podem ser consideradas ru\u00eddo para o modelo. Por isso, foram removidas. diabetes_dataset = diabetes_dataset.drop(columns=['ID', 'No_Pation']) 2.3. Codifica\u00e7\u00e3o de Vari\u00e1veis Categ\u00f3ricas Modelos de machine learning requerem que todas as features de entrada sejam num\u00e9ricas. Portanto, converti as colunas categ\u00f3ricas Gender e CLASS . Obs.: Em determinados algoritmos, essa parte n\u00e3o interfere tanto mas deixei a men\u00e7\u00e3o para quando for usada nos algoritmos. Gender (One-Hot Encoding) : Para a coluna Gender , que \u00e9 uma vari\u00e1vel nominal (n\u00e3o h\u00e1 uma ordem intr\u00ednseca entre 'M' e 'F'), utilizei a t\u00e9cnica One-Hot Encoding com pd.get_dummies . Isso cria novas colunas bin\u00e1rias ( Gender_F e Gender_M ), evitando que o modelo atribua um peso ordinal indevido. df_diabetes = pd.get_dummies(diabetes_dataset, columns=['Gender'], dtype=int) CLASS (Label Encoding) : A coluna CLASS \u00e9 a nossa vari\u00e1vel-alvo (target). Utilizei o LabelEncoder da biblioteca scikit-learn para transform\u00e1-la em valores num\u00e9ricos (0, 1, 2). Essa \u00e9 uma abordagem padr\u00e3o para a vari\u00e1vel de sa\u00edda em problemas de classifica\u00e7\u00e3o. from sklearn.preprocessing import LabelEncoder le = LabelEncoder() df_diabetes['CLASS'] = le.fit_transform(df_diabetes['CLASS']) # Verificando o mapeamento das classes # Sa\u00edda: Class Map: [(0, 'N'), (1, 'P'), (2, 'Y')] 3. Prepara\u00e7\u00e3o dos Dados para a Modelagem Antes de treinar qualquer modelo, \u00e9 crucial preparar os dados corretamente, tratando quest\u00f5es como desbalanceamento de classes e a escala das features. 3.1. Visualiza\u00e7\u00e3o da Distribui\u00e7\u00e3o das Classes Plotei um gr\u00e1fico para visualizar a distribui\u00e7\u00e3o das classes e identificamos um forte desbalanceamento . A classe 'Y' (diab\u00e9tico) era massivamente majorit\u00e1ria, o que pode enviesar o modelo a prever sempre essa classe, ignorando as minorit\u00e1rias ('N' e 'P'). 3.2. Divis\u00e3o em Dados de Treino e Teste Dividi o dataset em conjuntos de treino (80%) e teste (20%). Usamos o par\u00e2metro stratify=y para garantir que a propor\u00e7\u00e3o das classes nos conjuntos de treino e teste fosse a mesma do dataset original, o que \u00e9 fundamental em casos de dados desbalanceados. from sklearn.model_selection import train_test_split X = df_diabetes.drop('CLASS', axis=1) y = df_diabetes['CLASS'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y) 3.3. Balanceamento das Classes com SMOTE Para lidar com o desbalanceamento, apliquei a t\u00e9cnica SMOTE (Synthetic Minority Over-sampling Technique) . O SMOTE cria novas amostras sint\u00e9ticas das classes minorit\u00e1rias ('N' e 'P') no conjunto de treino , baseando-se nas amostras existentes. Isso permite que o modelo aprenda as caracter\u00edsticas de todas as classes de forma mais equilibrada. Importante : O SMOTE foi aplicado apenas nos dados de treino para evitar vazamento de dados (data leakage), garantindo que o conjunto de teste permane\u00e7a uma representa\u00e7\u00e3o real e \"in\u00e9dita\" do problema. from imblearn.over_sampling import SMOTE from collections import Counter print(\"Distribui\u00e7\u00e3o antes do SMOTE:\", Counter(y_train)) # Sa\u00edda: Class distribution before SMOTE: Counter({2: 675, 0: 82, 1: 43}) smote = SMOTE(random_state=42) X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train) print(\"\\nDistribui\u00e7\u00e3o ap\u00f3s o SMOTE:\", Counter(y_train_resampled)) # Sa\u00edda: Class Distribution after SMOTE: Counter({2: 675, 0: 675, 1: 675}) 3.4. Escalonamento das Features (Feature Scaling) Muitos algoritmos de Machine Learning (como Regress\u00e3o Log\u00edstica, SVM, Redes Neurais, etc.) s\u00e3o sens\u00edveis \u00e0 escala das features. Uma feature com uma grande amplitude de valores (ex: Cr ) poderia dominar outras com valores menores (ex: Urea ), distorcendo o aprendizado. Para evitar isso, usei o StandardScaler , que padroniza as features para que tenham m\u00e9dia 0 e desvio padr\u00e3o 1. O scaler foi treinado ( fit_transform ) com os dados de treino e depois apenas aplicado ( transform ) nos dados de teste, novamente para evitar vazamento de informa\u00e7\u00f5es do conjunto de teste para o modelo. from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train_resampled) X_test_scaled = scaler.transform(X_test) 4. Conclus\u00e3o do Pr\u00e9-Processamento e Pr\u00f3ximos Passos Ao final desta detalhada etapa de pr\u00e9-processamento, os dados est\u00e3o limpos, balanceados e padronizados, prontos para serem utilizados na fase de modelagem. As principais transforma\u00e7\u00f5es realizadas foram: Limpeza e Padroniza\u00e7\u00e3o : Corre\u00e7\u00e3o de inconsist\u00eancias nos dados categ\u00f3ricos. Codifica\u00e7\u00e3o de Features : Convers\u00e3o de vari\u00e1veis categ\u00f3ricas em formato num\u00e9rico. Balanceamento de Classes : Mitiga\u00e7\u00e3o do vi\u00e9s da classe majorit\u00e1ria usando SMOTE. Escalonamento de Features : Normaliza\u00e7\u00e3o da escala das vari\u00e1veis para um tratamento justo pelos algoritmos. Com esta base s\u00f3lida e confi\u00e1vel, os pr\u00f3ximos passos envolvem treinar e avaliar diferentes algoritmos utilizando os conjuntos X_train_scaled , y_train_resampled , X_test_scaled e y_test (para casos onde esses subconjuntos precisam ser normalizados e bem distribu\u00eddos) para determinar qual modelo oferece a melhor performance para este problema.","title":"O Dataset"},{"location":"the_dataset/#descricao-do-dataset","text":"O conjunto de dados utilizado neste projeto foi obtido na plataforma Kaggle, disponibilizado pelo autor Marshal Patel. Este dataset \u00e9 composto por informa\u00e7\u00f5es biom\u00e9dicas e caracter\u00edsticas de pacientes, sendo fundamental para a cria\u00e7\u00e3o de modelos de predi\u00e7\u00e3o de diabetes. O dataset pode ser acessado clicado nesse link Link do Dataset :","title":"Descri\u00e7\u00e3o do Dataset"},{"location":"the_dataset/#descricao-dos-parametros","text":"Abaixo, segue uma descri\u00e7\u00e3o detalhada de cada um dos par\u00e2metros (colunas) presentes no conjunto de dados: Gender: O g\u00eanero do paciente (Feminino ou Masculino). AGE: A idade do paciente em anos. Urea: O n\u00edvel de ureia no sangue, um indicador da fun\u00e7\u00e3o renal. Cr (Creatinina): O n\u00edvel de creatinina no sangue, outro importante indicador da fun\u00e7\u00e3o dos rins. HbA1c (Hemoglobina Glicada): Um exame que mede a m\u00e9dia dos n\u00edveis de a\u00e7\u00facar no sangue nos \u00faltimos 2 a 3 meses. \u00c9 um par\u00e2metro chave para o diagn\u00f3stico e monitoramento do diabetes. Chol (Colesterol): O n\u00edvel de colesterol total no sangue. TG (Triglicer\u00eddeos): O n\u00edvel de triglicer\u00eddeos no sangue, um tipo de gordura presente na corrente sangu\u00ednea. HDL (Lipoprote\u00edna de Alta Densidade): Conhecido como \"colesterol bom\", ajuda a remover o excesso de colesterol do corpo. LDL (Lipoprote\u00edna de Baixa Densidade): Conhecido como \"colesterol ruim\", seu ac\u00famulo pode levar \u00e0 forma\u00e7\u00e3o de placas nas art\u00e9rias. VLDL (Lipoprote\u00edna de Muito Baixa Densidade): Um precursor do LDL, tamb\u00e9m associado ao ac\u00famulo de placas nas art\u00e9rias. BMI (\u00cdndice de Massa Corporal): Uma medida da gordura corporal baseada na altura e no peso do indiv\u00edduo. CLASS: A classe de diagn\u00f3stico do paciente, podendo ser N (N\u00e3o diab\u00e9tico), P (Pr\u00e9-diab\u00e9tico) ou Y (Diab\u00e9tico). Este dataset foi utilizado em todas as t\u00e9cnicas do meu projeto como base para explorar as possibilidades.","title":"Descri\u00e7\u00e3o dos Par\u00e2metros"},{"location":"the_dataset/#analise-de-dados-e-preparacao-para-modelagem-de-predicao-de-diabetes","text":"Aqui eu fa\u00e7o uma descri\u00e7\u00e3o que descreve o processo completo de an\u00e1lise e pr\u00e9-processamento de um dataset de predi\u00e7\u00e3o de diabetes. O objetivo \u00e9 preparar os dados de forma robusta, criando uma base s\u00f3lida e confi\u00e1vel para o treinamento de diferentes algoritmos de Machine Learning capazes de classificar pacientes como n\u00e3o-diab\u00e9ticos (N), pr\u00e9-diab\u00e9ticos (P) ou diab\u00e9ticos (Y).","title":"An\u00e1lise de Dados e Prepara\u00e7\u00e3o para Modelagem de Predi\u00e7\u00e3o de Diabetes"},{"location":"the_dataset/#1-aquisicao-e-exploracao-dos-dados","text":"O primeiro passo em qualquer projeto de ci\u00eancia de dados \u00e9 obter e entender o conjunto de dados com o qual estamos trabalhando.","title":"1. Aquisi\u00e7\u00e3o e Explora\u00e7\u00e3o dos Dados"},{"location":"the_dataset/#11-carregamento-do-dataset","text":"Utilizei a biblioteca opendatasets para baixar o conjunto de dados diretamente da plataforma Kaggle e o pandas para carreg\u00e1-lo em um DataFrame, que \u00e9 a estrutura de dados fundamental para an\u00e1lise em Python. import opendatasets as od import pandas as pd od.download(\"https://www.kaggle.com/datasets/marshalpatel3558/diabetes-prediction-dataset-legit-dataset\")","title":"1.1. Carregamento do Dataset"},{"location":"the_dataset/#12-analise-exploratoria-inicial","text":"Com os dados carregados, realizei uma an\u00e1lise explorat\u00f3ria inicial para entender sua estrutura, tipos de dados e a presen\u00e7a de valores ausentes. .head() : Visualizei as 5 primeiras linhas para ter uma primeira impress\u00e3o dos dados e das colunas. diabetes_dataset.head(5) ID No_Pation Gender AGE Urea Cr HbA1c Chol TG HDL LDL VLDL BMI CLASS 0 502 17975 F 50 4.7 46 4.9 4.2 0.9 2.4 1.4 0.5 24.0 N 1 735 34221 M 26 4.5 62 4.9 3.7 1.4 1.1 2.1 0.6 23.0 N 2 420 47975 F 50 4.7 46 4.9 4.2 0.9 2.4 1.4 0.5 24.0 N 3 680 87656 F 50 4.7 46 4.9 4.2 0.9 2.4 1.4 0.5 24.0 N 4 504 34223 M 33 7.1 46 4.9 4.9 1.0 0.8 2.0 0.4 21.0 N .info() : Verifiquei os tipos de dados de cada coluna e a contagem de valores n\u00e3o-nulos. Felizmente, o dataset n\u00e3o apresentou valores ausentes, o que simplifica a etapa de limpeza. diabetes_dataset.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 1000 entries, 0 to 999 Data columns (total 14 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 1000 non-null int64 1 No_Pation 1000 non-null int64 2 Gender 1000 non-null object 3 AGE 1000 non-null int64 4 Urea 1000 non-null float64 5 Cr 1000 non-null int64 6 HbA1c 1000 non-null float64 7 Chol 1000 non-null float64 8 TG 1000 non-null float64 9 HDL 1000 non-null float64 10 LDL 1000 non-null float64 11 VLDL 1000 non-null float64 12 BMI 1000 non-null float64 13 CLASS 1000 non-null object","title":"1.2. An\u00e1lise Explorat\u00f3ria Inicial"},{"location":"the_dataset/#2-pre-processamento-e-limpeza-dos-dados","text":"A qualidade do modelo de Machine Learning depende diretamente da qualidade dos dados de entrada. Nesta etapa, corrigi inconsist\u00eancias e transformei os dados para um formato adequado para o treinamento.","title":"2. Pr\u00e9-processamento e Limpeza dos Dados"},{"location":"the_dataset/#21-limpeza-de-dados-categoricos","text":"identifiquei que as colunas categ\u00f3ricas Gender e CLASS continham ru\u00eddos, como valores diferentes para a mesma categoria (ex: 'N' e 'N ') e inconsist\u00eancia de mai\u00fasculas/min\u00fasculas (ex: 'F' e 'f'). # Verificando os valores \u00fanicos antes da limpeza list_class = diabetes_dataset['CLASS'].unique() list_gender = diabetes_dataset['Gender'].unique() print(f\"For label CLASS: {list_class}\\nFor label Gender: {list_gender}\") # Sa\u00edda: # For label CLASS: ['N' 'N ' 'P' 'Y' 'Y '] # For label Gender: ['F' 'M' 'f'] Para corrigir, apliquei uma fun\u00e7\u00e3o para converter todos os valores para mai\u00fasculas e remover espa\u00e7os em branco no in\u00edcio e no fim das strings. # Aplicando a limpeza diabetes_dataset['CLASS'] = diabetes_dataset['CLASS'].apply(lambda x: str.upper(x).strip()) diabetes_dataset['Gender'] = diabetes_dataset['Gender'].apply(lambda x: str.upper(x)) # Verificando o resultado ap\u00f3s a limpeza list_class = diabetes_dataset['CLASS'].unique() list_gender = diabetes_dataset['Gender'].unique() print(f\"For label CLASS: {list_class}\\nFor label Gender: {list_gender}\") # Sa\u00edda: # For label CLASS: ['N' 'P' 'Y'] # For label Gender: ['F' 'M']","title":"2.1. Limpeza de Dados Categ\u00f3ricos"},{"location":"the_dataset/#22-remocao-de-colunas-irrelevantes","text":"As colunas ID e No_Pation s\u00e3o identificadores \u00fanicos para cada paciente. Elas n\u00e3o possuem valor preditivo e podem ser consideradas ru\u00eddo para o modelo. Por isso, foram removidas. diabetes_dataset = diabetes_dataset.drop(columns=['ID', 'No_Pation'])","title":"2.2. Remo\u00e7\u00e3o de Colunas Irrelevantes"},{"location":"the_dataset/#23-codificacao-de-variaveis-categoricas","text":"Modelos de machine learning requerem que todas as features de entrada sejam num\u00e9ricas. Portanto, converti as colunas categ\u00f3ricas Gender e CLASS . Obs.: Em determinados algoritmos, essa parte n\u00e3o interfere tanto mas deixei a men\u00e7\u00e3o para quando for usada nos algoritmos. Gender (One-Hot Encoding) : Para a coluna Gender , que \u00e9 uma vari\u00e1vel nominal (n\u00e3o h\u00e1 uma ordem intr\u00ednseca entre 'M' e 'F'), utilizei a t\u00e9cnica One-Hot Encoding com pd.get_dummies . Isso cria novas colunas bin\u00e1rias ( Gender_F e Gender_M ), evitando que o modelo atribua um peso ordinal indevido. df_diabetes = pd.get_dummies(diabetes_dataset, columns=['Gender'], dtype=int) CLASS (Label Encoding) : A coluna CLASS \u00e9 a nossa vari\u00e1vel-alvo (target). Utilizei o LabelEncoder da biblioteca scikit-learn para transform\u00e1-la em valores num\u00e9ricos (0, 1, 2). Essa \u00e9 uma abordagem padr\u00e3o para a vari\u00e1vel de sa\u00edda em problemas de classifica\u00e7\u00e3o. from sklearn.preprocessing import LabelEncoder le = LabelEncoder() df_diabetes['CLASS'] = le.fit_transform(df_diabetes['CLASS']) # Verificando o mapeamento das classes # Sa\u00edda: Class Map: [(0, 'N'), (1, 'P'), (2, 'Y')]","title":"2.3. Codifica\u00e7\u00e3o de Vari\u00e1veis Categ\u00f3ricas"},{"location":"the_dataset/#3-preparacao-dos-dados-para-a-modelagem","text":"Antes de treinar qualquer modelo, \u00e9 crucial preparar os dados corretamente, tratando quest\u00f5es como desbalanceamento de classes e a escala das features.","title":"3. Prepara\u00e7\u00e3o dos Dados para a Modelagem"},{"location":"the_dataset/#31-visualizacao-da-distribuicao-das-classes","text":"Plotei um gr\u00e1fico para visualizar a distribui\u00e7\u00e3o das classes e identificamos um forte desbalanceamento . A classe 'Y' (diab\u00e9tico) era massivamente majorit\u00e1ria, o que pode enviesar o modelo a prever sempre essa classe, ignorando as minorit\u00e1rias ('N' e 'P').","title":"3.1. Visualiza\u00e7\u00e3o da Distribui\u00e7\u00e3o das Classes"},{"location":"the_dataset/#32-divisao-em-dados-de-treino-e-teste","text":"Dividi o dataset em conjuntos de treino (80%) e teste (20%). Usamos o par\u00e2metro stratify=y para garantir que a propor\u00e7\u00e3o das classes nos conjuntos de treino e teste fosse a mesma do dataset original, o que \u00e9 fundamental em casos de dados desbalanceados. from sklearn.model_selection import train_test_split X = df_diabetes.drop('CLASS', axis=1) y = df_diabetes['CLASS'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)","title":"3.2. Divis\u00e3o em Dados de Treino e Teste"},{"location":"the_dataset/#33-balanceamento-das-classes-com-smote","text":"Para lidar com o desbalanceamento, apliquei a t\u00e9cnica SMOTE (Synthetic Minority Over-sampling Technique) . O SMOTE cria novas amostras sint\u00e9ticas das classes minorit\u00e1rias ('N' e 'P') no conjunto de treino , baseando-se nas amostras existentes. Isso permite que o modelo aprenda as caracter\u00edsticas de todas as classes de forma mais equilibrada. Importante : O SMOTE foi aplicado apenas nos dados de treino para evitar vazamento de dados (data leakage), garantindo que o conjunto de teste permane\u00e7a uma representa\u00e7\u00e3o real e \"in\u00e9dita\" do problema. from imblearn.over_sampling import SMOTE from collections import Counter print(\"Distribui\u00e7\u00e3o antes do SMOTE:\", Counter(y_train)) # Sa\u00edda: Class distribution before SMOTE: Counter({2: 675, 0: 82, 1: 43}) smote = SMOTE(random_state=42) X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train) print(\"\\nDistribui\u00e7\u00e3o ap\u00f3s o SMOTE:\", Counter(y_train_resampled)) # Sa\u00edda: Class Distribution after SMOTE: Counter({2: 675, 0: 675, 1: 675})","title":"3.3. Balanceamento das Classes com SMOTE"},{"location":"the_dataset/#34-escalonamento-das-features-feature-scaling","text":"Muitos algoritmos de Machine Learning (como Regress\u00e3o Log\u00edstica, SVM, Redes Neurais, etc.) s\u00e3o sens\u00edveis \u00e0 escala das features. Uma feature com uma grande amplitude de valores (ex: Cr ) poderia dominar outras com valores menores (ex: Urea ), distorcendo o aprendizado. Para evitar isso, usei o StandardScaler , que padroniza as features para que tenham m\u00e9dia 0 e desvio padr\u00e3o 1. O scaler foi treinado ( fit_transform ) com os dados de treino e depois apenas aplicado ( transform ) nos dados de teste, novamente para evitar vazamento de informa\u00e7\u00f5es do conjunto de teste para o modelo. from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train_resampled) X_test_scaled = scaler.transform(X_test)","title":"3.4. Escalonamento das Features (Feature Scaling)"},{"location":"the_dataset/#4-conclusao-do-pre-processamento-e-proximos-passos","text":"Ao final desta detalhada etapa de pr\u00e9-processamento, os dados est\u00e3o limpos, balanceados e padronizados, prontos para serem utilizados na fase de modelagem. As principais transforma\u00e7\u00f5es realizadas foram: Limpeza e Padroniza\u00e7\u00e3o : Corre\u00e7\u00e3o de inconsist\u00eancias nos dados categ\u00f3ricos. Codifica\u00e7\u00e3o de Features : Convers\u00e3o de vari\u00e1veis categ\u00f3ricas em formato num\u00e9rico. Balanceamento de Classes : Mitiga\u00e7\u00e3o do vi\u00e9s da classe majorit\u00e1ria usando SMOTE. Escalonamento de Features : Normaliza\u00e7\u00e3o da escala das vari\u00e1veis para um tratamento justo pelos algoritmos. Com esta base s\u00f3lida e confi\u00e1vel, os pr\u00f3ximos passos envolvem treinar e avaliar diferentes algoritmos utilizando os conjuntos X_train_scaled , y_train_resampled , X_test_scaled e y_test (para casos onde esses subconjuntos precisam ser normalizados e bem distribu\u00eddos) para determinar qual modelo oferece a melhor performance para este problema.","title":"4. Conclus\u00e3o do Pr\u00e9-Processamento e Pr\u00f3ximos Passos"},{"location":"supervised_learning/knn/","text":"KNN","title":"KNN"},{"location":"supervised_learning/knn/#knn","text":"","title":"KNN"},{"location":"supervised_learning/mlp/","text":"MLP","title":"MLP"},{"location":"supervised_learning/mlp/#mlp","text":"","title":"MLP"},{"location":"supervised_learning/random_florest/","text":"Random Florest","title":"Random Florest"},{"location":"supervised_learning/random_florest/#random-florest","text":"","title":"Random Florest"},{"location":"supervised_learning/svm/","text":"SVM","title":"SVM"},{"location":"supervised_learning/svm/#svm","text":"","title":"SVM"},{"location":"supervised_learning/xgboost/","text":"XGBoost","title":"XGBoost"},{"location":"supervised_learning/xgboost/#xgboost","text":"","title":"XGBoost"},{"location":"unsupervised_learning/k_means/","text":"K-Means","title":"K-Means"},{"location":"unsupervised_learning/k_means/#k-means","text":"","title":"K-Means"}]}